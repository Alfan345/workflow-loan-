name: MLflow Model Training

on:
  push:
    branches: [ main, develop ]
    paths: 
      - 'modelling/**'
      - 'preprocessed_loan_data.csv'
  pull_request:
    branches: [ main ]
  workflow_dispatch:

jobs:
  model-training:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python 3.9
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Check required files
      run: |
        echo "=== Checking Required Files ==="
        files=("modelling/model_functions.py" "modelling/data_preparation.py" "modelling/modelling.py" "modelling/hyperparameter_tuning.py" "modelling/model_export.py" "preprocessed_loan_data.csv")
        
        for file in "${files[@]}"; do
          if [ -f "$file" ]; then
            echo "‚úÖ $file found"
          else
            echo "‚ùå $file not found"
            exit 1
          fi
        done
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pandas scikit-learn numpy mlflow matplotlib seaborn joblib
    
    - name: Setup directories
      run: |
        cd modelling
        mkdir -p mlruns
        mkdir -p ../models
        echo "‚úÖ Directories created"
    
    - name: Run your existing modelling script
      run: |
        cd modelling
        echo "üöÄ Running your modelling.py script..."
        python modelling.py
    
    - name: Check training results
      run: |
        echo "=== Training Results ==="
        
        # Check MLflow artifacts
        if [ -d "modelling/mlruns" ]; then
          echo "‚úÖ MLflow tracking data created"
          echo "Generated files:"
          find modelling/mlruns -name "*.png" -o -name "*.csv" -o -name "*.pkl" | head -10
        else
          echo "‚ùå No MLflow artifacts found"
        fi
        
        # Check exported models
        if [ -d "models" ]; then
          echo "‚úÖ Models exported to models directory"
          ls -la models/
        else
          echo "‚ùå No models directory found"
        fi
        
        # Check generated plots and reports
        cd modelling
        if ls confusion_matrix_*.png 1> /dev/null 2>&1; then
          echo "‚úÖ Confusion matrix plots generated"
          ls -la confusion_matrix_*.png
        fi
        
        if ls classification_report_*.csv 1> /dev/null 2>&1; then
          echo "‚úÖ Classification reports generated" 
          ls -la classification_report_*.csv
        fi
    
    - name: Show experiment summary
      run: |
        cd modelling
        echo "üìä Experiment Summary:"
        python -c "
        import mlflow
        from mlflow.tracking import MlflowClient
        
        try:
            mlflow.set_tracking_uri('file:./mlruns')
            client = MlflowClient()
            
            experiments = client.list_experiments()
            for exp in experiments:
                if exp.name != 'Default':
                    runs = client.search_runs(exp.experiment_id)
                    print(f'üìà Experiment: {exp.name}')
                    print(f'   Total runs: {len(runs)}')
                    
                    if runs:
                        # Sort by accuracy
                        sorted_runs = sorted(runs, key=lambda r: r.data.metrics.get('test_accuracy', r.data.metrics.get('accuracy', 0)), reverse=True)
                        
                        print('   Top 3 runs:')
                        for i, run in enumerate(sorted_runs[:3]):
                            metrics = run.data.metrics
                            accuracy = metrics.get('test_accuracy', metrics.get('accuracy', 0))
                            precision = metrics.get('test_precision', metrics.get('precision', 0))
                            recall = metrics.get('test_recall', metrics.get('recall', 0))
                            f1 = metrics.get('test_f1_score', metrics.get('f1_score', 0))
                            
                            print(f'     {i+1}. Run {run.info.run_id[:8]}:')
                            print(f'        Accuracy: {accuracy:.4f}')
                            print(f'        Precision: {precision:.4f}')
                            print(f'        Recall: {recall:.4f}')
                            print(f'        F1: {f1:.4f}')
                    print()
        except Exception as e:
            print(f'Error reading experiment data: {e}')
        "
    
    - name: Upload MLflow artifacts
      uses: actions/upload-artifact@v3
      with:
        name: mlflow-tracking-data
        path: modelling/mlruns/
        retention-days: 30
    
    - name: Upload generated reports and plots
      uses: actions/upload-artifact@v3
      with:
        name: model-reports
        path: |
          modelling/confusion_matrix_*.png
          modelling/classification_report_*.csv
        retention-days: 30
    
    - name: Upload exported models
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: exported-models
        path: models/
        retention-days: 90
    
    - name: Workflow completion
      run: |
        echo "üéâ Model Training Workflow Completed!"
        echo "================================================"
        echo "‚úÖ Used your existing scripts without modification:"
        echo "   - modelling.py (main training script)"
        echo "   - model_functions.py (training & logging functions)"
        echo "   - hyperparameter_tuning.py (parameter optimization)"
        echo "   - model_export.py (model export functions)"
        echo "   - data_preparation.py (data loading & preprocessing)"
        echo ""
        echo "üìÅ Artifacts uploaded:"
        echo "   - MLflow tracking data (experiments, metrics, models)"
        echo "   - Model reports (confusion matrix, classification reports)"
        echo "   - Exported models (ready for deployment)"
        echo ""
        echo "üîç Download artifacts from the 'Actions' tab to explore results"